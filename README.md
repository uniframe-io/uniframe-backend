# uniframe-backend


## Naming Styles
The table below outlines some of the common naming styles in Python code and when you should use them:

Type | Naming Convention | Examples
-- | -- | --
Function | Use a lowercase word or words. Separate words by underscores to improve readability. | function, my_function
Variable | Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. | x, var, my_variable
Class | Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. | Model, MyClass
Method | Use a lowercase word or words. Separate words with underscores to improve readability. | class_method, method
Constant | Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. | CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT
Module | Use a short, lowercase word or words. Separate words with underscores to improve readability. | module.py, my_module.py
Package | Use a short, lowercase word or words. Do not separate words with underscores. | package, mypackage

## Git flow
- we only use main and feature branches development for now
- We follow gitflow name convention for branch name
- Commit message: start with issue No
- For PR name and content, follow the PR template
- For issue name and content, follow the issue template

## Continue Integration
before push, run black, isort, flake8, mypy and pytest
```sh
isort solution
black -l 80 solution
flake8 --config setup.cfg solution
mypy solution
API_RUN_LOCATION=test pytest api_tests/
```

## Continue Deployment

Dokcerfile is used for local hosting. Dockerfile.deploy is used for deploying on cloud environment.

**N.B.** The AWS resources are provisoned by [infra repository ](https://github.com/uniframe-io/name-matching-infra)


Currently, we have two ways to deploy:
- automatic deploy: We use Github Action for continuous deployment. The configuration file is `.github/workflows/cd-ecs-{env}.yml`. When a branch is merged to main, the dev environment will be deployed.
- manual deploy: Run `scripts/deploy/manual_deploy_dev.sh` script to deploy to dev environment


Environment environment varible which used by the application listed below. They are configured by either docker build or ECS task definitions. 


| Env variable  | Explanation  | Remote Github Action deploy | Remote manual script deploy  | Local CI test and local docker-compose  |
|---|---|---|---|---|
| **PRODUCT_PREFIX=uniframe**  | the product prefix of all infrastructure resource  | hardcoded id cd yaml, build into image by dockerfile.deploy | hardcoded in script, build into image by dockerfile.deploy |configure in .env file, imported by docker-compose env_file section  |
| **DEPLOY_ENV**  | the deployment environment. Currently, we only have `dev`. `qa` and `prod` will be added in future, and **DEPLOY_ENV** will also be used for multi-tenancy  | hardcoded in cd yaml, build into image by dockerfile.deploy | hardcoded in cd yaml, build into image by dockerfile.deploy  | configure in .env file, imported by docker-compose env_file section  |
| **AWS_ACCOUNT_ID**  | the AWS account deployed the services  | hardcoded, used for retrieve the cd IAM user | `aws configure` | No need. AWS_ACCESS_KEY_ID cover it |
| **AWS_ENV_REGION**  | the AWS region deployed the services  | hardcoded  | covered by `aws configure` default region | covered by `aws configure` default region |
| **AWS_ACCESS_KEY_ID**  | AWS access key and secret  |  the accessretieved from Github secret, pre-config manually in Github company organization. Use for CD aws command | In `aws configure`, used for call aws command in manual script | hardcoded in `.env` file, which load into container as environemnt variable. The local ci-test or docker-compose server will use it to communicate with AWS service (s3, etc) |
| **AWS_SECRET_ACCESS_KEY**  | AWS access key and secret  |  the accessretieved from Github secret, pre-config manually in Github company organization. Use for CD aws command | In `aws configure`, used for call aws command in manual script | hardcoded in `.env` file, which load into container as environemnt variable. The local ci-test or docker-compose server will use it to communicate with AWS service (s3, etc) |
| **DOMAIN_NAME**  | the applicatio domain  | retrieved from parameter store, which was generated by infra CDK deployment, build into image by dockerfile.deploy  | retrieved from parameter store, which was generated by infra CDK deployment, build into image by dockerfile.deploy | hardcoded to `localhost` |
| **POSTGRES_USER**  | backend PG user  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh`  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh` | No need to setup. Hardcoded is `api_setting.py` |
| **POSTGRES_PASSWORD**  | backend PG password  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh`  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh` | No need to setup. Hardcoded is `api_setting.py` |
| **POSTGRES_HOST**  | backend PG hostname  | retrieved from parameter store, and setup to environment variable in container by `entrypoint.sh`  | retrieved from parameter store, and setup to environment variable in container by `entrypoint.sh` | No need to setup. Hardcoded is `api_setting.py` |
| **POSTGRES_DB**  | backend PG databse  | retrieved from parameter store, and setup to environment variable in container by `entrypoint.sh`  | retrieved from parameter store, and setup to environment variable in container by `entrypoint.sh` | No need to setup. Hardcoded is `api_setting.py` |
| **OAUTH2_GITHUB_CLIENT_ID**  | Github Oauth2 client id  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh`  |  retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh` | setup a dummy value in `.env` file, imported by docker-compose env_file section|
| **OAUTH2_GITHUB_CLIENT_SECRET**  | Github Oauth2 client id  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh`  |  retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh` | setup a dummy value in `.env` file, imported by docker-compose env_file section|
| **API_JWT_TOKEN_SECRET**  | backend JWT token encode/decode secret  | retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh`  |  retrieved from secret manager, and setup to environment variable in container by `entrypoint.sh` | hardcoded in `.env` file, imported by docker-compose env_file section|
| **AWS_CD_ROLE**: the deployment role, able to run Github action cd pipeline | hardcoded in cd pipeline and used | N/A | N/A | N/A |



### Local testing
We can also deploy the backend server locally
- docker-compose: use command `make docker-backend`. It will use docker-compose to start backend and worker. The PG db `nm` is connected
- local ci-test: use command `make ci-test`. It will use It will use docker-compose to start backend and worker. The PG db `test_nm` is connected

| Env variable  | Explanation  | Local CI test | local docker-compose  |
|---|---|---|---|
| **API_RUN_LOCATION**  | backend runing location: `test` or `local`. Setup in Makefile  |  setup the value as `test` |  setup the value as `local` |


## Useful Commands
Some useful commands are build by makefile. Please add your commands into `Makefile` file under the root folder
``` sh
# run all CI commands, including isort, black, flake8, mypy and pytest
make ci-test

# run name matching main api server by docker-compose
make docker-api

# host mkdocs at http://localhost:8002
make host-docs
```


## Documentation
### File, class, function docstring
- Use docstring to comment class and functions. Use pdoc to generate the document automatically
- docstring: at least have these sections
    - function/class description
    - arguments and return value: type and description
    - any difficult part of the code

### FastAPI endpoint docstring
This [FastAPI document](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#description-from-docstring) describes the docstring. In short:
- In @app decorators, use `summary="some text"` to give a summary of the endpoint
- In @app decorator, use `response_description="some text"` to describe the response
- docstring can use `**` for bold font.

Here is an example from FastAPI document
``` python
@app.post(
    "/items/",
    response_model=Item,
    summary="Create an item",
    response_description="The created item",
)
async def create_item(item: Item):
    """
    Create an item with all the information:

    - **name**: each item must have a name
    - **description**: a long description
    - **price**: required
    - **tax**: if the item doesn't have tax, you can omit this
    - **tags**: a set of unique tag strings for this item
    """
    return item
```

## Run python script
In order to import packages correctly
1. run `pip install -e .`
2. run python script or pytest on the root folder. For example, `pytest solution/tests/metadata/test_job_config.py`

## TODO tag
use `TODO` tag in comments or docstring for features or functionality which will be added or extended in future

## Error and Exception
The customized exeption class is defined in `solution/err_def.py`. Please use it in `raise` statement.

In FastAPI endpoint, add this function to override the HTTP response when handling the customized exception
``` python
@app.exception_handler(NmBaseException)
async def api_exception_handler(request, exc: NmBaseException):
    return JSONResponse(
        status_code=exc.status_code, content=exc.content_to_dict()
    )
```

## Logging
There is a customer logger in `solution/settings/logger.py`. The default configuration of the logger is in `conf/logger_cfg.yaml`.

By default, there will be a `nm.log` file generated under `log/` when the log level is more servere than *WARNING*. But this can be changed easily in the `logger_cfg.yaml` file.

**Logger needs to be initiated in order to be used**, for example
```yaml
loggers:
  fastapi:
    level: INFO
    handlers:
    - console
    - file
    propagate: false
  worker:
    level: INFO
    handlers:
    - console
    - file
    propagate: false
  test:
    level: INFO
    handlers:
    - console
    - file
    propagate: false  
```

The customer logger is needed to be imported in each module and initialized once.
```python
# some_module.py
from server.settings.logger import init_logging

logger = init_logging().getLogger('fastapi')

logger.info("info level")
logger.warning("warning level")
logger.error("error level")

```

The expected output in the CLI is
```bash
2020-12-16 00:35:58,542 some_module.py [INFO] info level
2020-12-16 00:35:58,542 some_module.py [WARNING] warning level
2020-12-16 00:35:58,542 some_module.py [ERROR] error level
```

## Installation psyco2pg on MacOS
``` sh
brew install postgresql
xcode-select --install
env LDFLAGS="-I/usr/local/opt/openssl/include -L/usr/local/opt/openssl/lib" pip install psycopg2
```

## Deployment for local and dev
### Installation docker-compose
[Install Docker Compose](https://docs.docker.com/compose/install/)

### Run FastAPI server by docker-compose
```
make docker-api
```

### Connect to pg
```
docker-compose exec db /bin/bash
psql -h db -U postgres
```

### Other commands
```
// stop container and rm pg volume
docker-compose down --volumes
```

### SSH into the ECS backend container
You can check the cluster name and task id in the console.
**Make sure your `aws cli` is up-to-date, version > 1.19.32**
```bash
aws ecs execute-command --region eu-west-1 \
  --cluster uniframe-dev-container-service-fargateclusterB9238D84-szQyDYqI6R33 \
  --task 88e2a005dc0345599244d393179bdd01 \
  --container fastapi \
  --interactive \
  --command "/bin/bash"
```

## Oauth2
### Login with GitHub
[Building OAuth Apps](https://docs.github.com/en/developers/apps/building-oauth-apps)


## SQLAlchemy Session Management
We borrow the implementation of [FastAPI-Sqlalchemy](https://github.com/mfreeborn/fastapi-sqlalchemy). The code below demos how to use Sqlalchemy session in and out a FastAPI request
`server/api/main.py`
``` python
from server.libs.db.sqlalchemy import DBSessionMiddleware, engine, session_args

app.add_middleware(
    DBSessionMiddleware, custom_engine=engine, session_args=session_args
)
```

In CRUD or endpoint file:
``` python
from server.libs.db.sqlalchemy import db

def crud_operation():
    db.session.query(...)
    db.session.commit
```

If we want to use sqlalchemy outside FastAPI request context
``` python
from server.libs.db.sqlalchemy import db

def other_function():
    with db():
        db.session.add(...)
        db.session.commit(...)
```

In pytest:
``` python
from server.libs.db.sqlalchemy import db

@pytest.fixture(scope="module", autouse=True)
def get_db() -> Any:
    with db():
        yield db
```

Handle Sqlalchemy exception
``` python
try:
    db.session.add(po_group)
    db.session.commit()
except exc.IntegrityError:
    db.session.rollback()
    raise EXCEPTION_LIB.GROUP__GROUP_OWNER_ID_NOT_IN_USER_TABLE.value(
        f"The group owner id {user_id} not in users table"
    )
```

## Alembic 

[Tutorial](https://alembic.sqlalchemy.org/en/latest/tutorial.html)

Alembic is a lightweight database migration tool for usage with the SQLAlchemy Database Toolkit for Python.

### Steps to run migration in local environment
1. Update Sqlalchemy Model 

update the model definition you want to change. e.g. model in `server/apps/user/models.py`

if you add a new model, don't forget to edit `alemic/env.py` and import the model after the block
```
from server.apps.user.models import User
from server.apps.dataset.models import (
    Dataset,
    DatasetShareGroup,
    DatasetShareUser,
)
from server.apps.group.models import Group, GroupMembers
from server.apps.media.models import Media
from server.apps.nm_task.models import AbcXyzTask
from server.apps.oauth.models import OAuth2User, VerificationCode
from server.apps.permission.models import LocalDeployUser
```

2. Generate migration file

```
make docker-backend
docker-compose exec server /bin/bash -c 'alembic revision --autogenerate -m "xxx"'
```

3. Run our migration

```
docker-compose exec server /bin/bash -c 'alembic upgrade head'
```

### Memo
1. After generated migration file, you need to add it to git repository.
2. Noted that when multiple people work on sqlachemy model, it must follow this way: only when first people finish the work on sqlachemy model and migrate by alembic, the other people can work on it
